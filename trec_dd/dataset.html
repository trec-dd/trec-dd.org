<!doctype html>
<html>
	<head>
		<meta charset="ISO-8859-1">
		<title>TREC 2016 Dynamic Domain Track</title>
                <link rel="stylesheet" type="text/css" href="css/style.css"/>
                <script type="text/javascript" src="js/jquery.min.js"></script>
                    <script type="text/javascript" src="js/jquery.pin.js"></script>
		<!--[if lt IE 9]>
		<script src="js/html5shiv.js"></script>
		<![endif]-->
		<!--<script type="text/javascript" src="js/main.js"></script>-->
	</head>

<body>

<div id = "top">
        <img class="logo" src="img/cube.png"/>
      <header>
        <br>
      
        TREC Dynamic Domain Track
        
      </header>
      
      <div class="line"></div>
</div>

<div id = "left">
        <nav>
        <a href="./index.html">Overview</a> 
        <a href="./timeline.html">Timeline</a> 
        <a href="./guideline.html">Guidelines</a> 
        <a href="./dataset.html">Datasets</a>
        <a href="https://groups.google.com/forum/#!forum/trec-dd">Google Group</a> 
        <a href="./ack.html">Acknowledgments</a>
        <a href="./2015.html">TREC DD 2015</a>
      </nav>
</div>
                        <div id="right">
                             <h4>TREC Dynamic Domain Track 2016 Datasets </h4>
<div class="overview">

</div>
<ul id="gdcontent">
<li><a href="#details">1. Datasets Descriptions</a>
<li><a href="#cca">2. CCA Generic Schema</a>
<li><a href="#format">3. Data Format</a>
<li><a href="#get">4. Obtaining the Datasets</a>
</ul>

            <section class="intro">

<div class="gdlinebox">
<h4><a name="details">1. Datasets Descriptions</a></h4>
<br>
    There are three datasets for the TREC 2016 Dynamic Domain Track. The Datasets include Weapon Dataset, Ebola Dataset, and Polar Dataset. Basic statistics of these datasets are shown in the Table 1-1. These datasets contains records in forms of web pages, scientific data, PDFs, and tweets. All the datasets are formatted using the Common Crawl Architecture schema from the DARPA MEMEX project, and stored as sequences of CBOR objects. Detailed Description, as well as some links to sample code, of each dataset is given in the following subsections.
<br>

<table style="width:90%">
  <caption>Table 1-1 Datasets Statistics</caption>
  <tr>
    <th>Datasets</th>
    <th>Size of Data on Disk</th>      
    <th>Number of Records</th>
  </tr>
  <tr>
    <td>Weapon</td>
    <td>~100 GB</td>        
    <td>3,345,557</td>
  </tr>
  <tr>
    <td>Ebola</td>
    <td>12.6 GB</td>        
    <td>682,157</td>
  </tr>
  <tr>
    <td>Polar</td>
    <td>158 GB</td>        
    <td>1,741,530</td>
  </tr>
</table>
<br>
<strong>1.1 Weapon Domain</strong><br>
<div class="gd_details">
This dataset was collected from U.S. law enforcement agencies, including a total of 3,345,557 documents. The dataset contains a list of the Defense Logistic Agency's microelectronics suppliers, a list of the buildings in China which are crowded with individual microelectronics suppliers, a list of elements that OSI/HSI are most interested in and are widely available on the counterfeit market, and a list of elements that are suspicious counterfeit. This dataset contains primarily corporate websites, merchant sites for ordering parts, social media sites, onion sites and some defunct sites.
<br>
<ul class="gd_details">
  <li> Company/POC for crawl: TBD</li>
  <li> Version: 1.0 </li>
  <li> Purpose of data: TBD.</li>
  <!-- <li> Description: This dataset has four disjoint parts: -->
<!--        <ol class="gd_details gdnum">
   <li>Ebola-web-01-2015: web pages crawled during January 2015.
   <li>Ebola-web-03-2015: web pages crawled during March 2015.  These two parts are primarily information from NGO's, relief agencies, and news organizations.
   <li>Ebola-pdfs: PDF documents collected from West African government and other sources.
   <li>Ebola-tweets: Tweets that originate from West African regions involved in the Ebola outbreak.
       </ol> -->
  <li> Schema notes: TBD
  <li> Number of items: 3,345,557 Documents.
  <li> Time frame: TBD
  <li> Geographic area: TBD
  <li> Size of data on disk: ~ 100GB
  <li> Format: TBD
</ul>
</div>
<br>
<strong>1.2 Ebola Domain</strong><br>
<div class="gd_details">
This data is related to the Ebola outbreak in Africa in 2014-2015. The dataset comprises tweets relating to the outbreak, web pages from sites hosted in the affected countries as well as PDF documents from websites such as The World Health Organization, Financial Tracking Service and The World Bank. Such information resources are designed to provide information to citizens and aid workers on the ground.<br>
<ul class="gd_details">
  <li> Company/POC for crawl: Juliana Friere and Kien Pham, NYU (juliana dot freire at nyu dot edu); Peter Landwehr 
peter dot landwehr at giantoak dot com); Lewis McGibbney, JPL (Lewis dot J dot Mcgibbney at jpl dor nasa dot gov)</li>
  <li> Version: 1.0 </li>
  <li> Purpose of data: represents an emerging humanitarian assistance situation.</li>
  <li> Four disjoint parts:
       <ol class="gd_details gdnum">
	 <li>Ebola-web-01-2015: web pages crawled during January 2015.
	 <li>Ebola-web-03-2015: web pages crawled during March 2015.  These two parts are primarily information from NGO's, relief agencies, and news organizations.
	 <li>Ebola-pdfs: PDF documents collected from West African government and other sources.
	 <li>Ebola-tweets: Tweets that originate from West African regions involved in the Ebola outbreak.
       </ol>
  <li> Time frame: January - March 2015
  <li> Geographic area: global, primarily West Africa
  <li> Size of data on disk: 12.6 GB
  <li> Format: Gzipped sequence of CBOR records; sequence of tweet pointers
  <li> Number of items: 497,362 web pages, 19,834 PDFs, 164,961 tweets.
  <li> Schema notes: The web and PDF subsets only contain the raw crawled data.  The tweets subset as distributed only contains pointers to tweets, because Twitter does not allow redistribution of tweets.  You will need to use the Twittertools crawler (link here) to get the actual tweets.  The output of that crawler includes both the HTML pages from Twitter, as well as extracted tweet data in the 'features' block.
</ul>
</div>
<br>

<strong>1.3 Polar Domain</strong><br>
<div class="gd_details">
This dataset is a set of web pages, scientific data, zip files, PDFs, images, and science code related to the polar sciences and available publicly from the NSF funded Advanced Cooperative Artic Data and Information System (ACADIS), NASA funded Antarctic Master Directory (AMD), and National Snow and Ice Data Center (NSIDC) Arctic Data Explorer. More information about this dataset can be found <a href="https://github.com/chrismattmann/trec-dd-polar/">here</a>.<br>
<ul class="gd_details">
  <li> Company/POC for crawl: Chris Mattman, JPL (chris dot a dot mattmann at jpl dot nasa dot gov)</li>
  <li> Version: 1.0 </li>
  <li> Purpose of data: represents a domain for open science and scientific data search.</li>
  <li> Description: Web pages, data files, zip archives, PDFs, images, and code.
  <li> Primary Crawling Sources: 
      <ol class="gd_details gdnum">
      <li>Dr. Chris Mattmann's crawl of ADE, performed at the Open Science Codefest and at the NSF DataViz Hackathon for Polar CyberInfrastructure</li>
      <li>Dr. Mattmann's student Angela Wang, contributed 3 datasets: 2 crawls of ACADIS and one of NASA AMD.</li>
      <li>Dr. Mattmann's CSCI 572 Course at USC, students submitted 13 individual crawls of NASA ACADIS, NSIDC ADE, and AMD.</li>
      </ol>
  
  <li> Number of items: 1,741,530 records.
  <li> Time frame: September 2014 - May 2015.
  <li> Geographic area: global
  <li> Size of data on disk: 158GB.
  <li> Format: Gzipped sequence of CBOR records, encrypted with the KBA StreamCorpus key. Crawled data were put into Common Crawl Format, acording to Memex format, using the <a href="http://wiki.apache.org/nutch/CommonCrawlDataDumper">CommonCrawlDataDumper</a>. The CommonCrawlDataDumper is an Apache Nutch tool that can dump Nutch segments into Common Crawl data format, mapping each crawled-by-Nutch file on a JSON-based data structure. CommonCrawlDataDumper dumps out the files and serialize them with CBOR encoding, a data representation format used in many contexts.Each contributed web crawl has an accompanying JSON file that lists the total records, by mimeType. A program, aggregate.py, aggregates all of the JSON files.
  <li> Schema notes: Here is a MIME type distribution breakdown of the data.
   <pre>
{
    "application/atom+xml": "2984",
    "application/dita+xml; format=concept": "345",
    "application/epub+zip": "36",
    "application/fits": "24",
    "application/gzip": "2060",
    "application/java-vm": "1",
    "application/msword": "244",
    "application/octet-stream": "211687",
    "application/ogg": "26",
    "application/pdf": "44658",
    "application/postscript": "219",
    "application/rdf+xml": "1042",
    "application/rss+xml": "8894",
    "application/rtf": "53",
    "application/vnd.google-earth.kml+xml": "298",
    "application/vnd.ms-excel": "227",
    "application/vnd.ms-excel.sheet.4": "1",
    "application/vnd.ms-htmlhelp": "1",
    "application/vnd.oasis.opendocument.presentation": "1",
    "application/vnd.oasis.opendocument.text": "10",
    "application/vnd.rn-realmedia": "105",
    "application/vnd.sun.xml.writer": "1",
    "application/x-7z-compressed": "2",
    "application/x-bibtex-text-file": "13",
    "application/x-bittorrent": "3",
    "application/x-bzip": "6",
    "application/x-bzip2": "63",
    "application/x-compress": "44",
    "application/x-debian-package": "4",
    "application/x-elc": "324",
    "application/x-executable": "35",
    "application/x-font-ttf": "9",
    "application/x-gtar": "46",
    "application/x-hdf": "41",
    "application/x-java-jnilib": "5",
    "application/x-lha": "2",
    "application/x-matroska": "66",
    "application/x-msdownload": "72",
    "application/x-msdownload; format=pe": "1",
    "application/x-msdownload; format=pe32": "16",
    "application/x-msmetafile": "6",
    "application/x-rar-compressed": "1",
    "application/x-rpm": "3",
    "application/x-sh": "5680",
    "application/x-shockwave-flash": "141",
    "application/x-sqlite3": "1",
    "application/x-stuffit": "1",
    "application/x-tar": "37",
    "application/x-tex": "17",
    "application/x-tika-msoffice": "2809",
    "application/x-tika-ooxml": "1775",
    "application/x-xz": "11",
    "application/xhtml+xml": "385751",
    "application/xml": "21000",
    "application/xslt+xml": "7",
    "application/zip": "3762",
    "audio/basic": "54",
    "audio/mp4": "18",
    "audio/mpeg": "646",
    "audio/vorbis": "5",
    "audio/x-aiff": "10",
    "audio/x-flac": "2",
    "audio/x-mpegurl": "1",
    "audio/x-ms-wma": "55",
    "audio/x-wav": "59",
    "image/gif": "40049",
    "image/jpeg": "85879",
    "image/png": "37997",
    "image/svg+xml": "342",
    "image/tiff": "477",
    "image/vnd.adobe.photoshop": "4",
    "image/vnd.dwg": "3",
    "image/vnd.microsoft.icon": "1570",
    "image/x-bpg": "7",
    "image/x-ms-bmp": "59",
    "image/x-xcf": "1",
    "message/rfc822": "182",
    "message/x-emlx": "1",
    "text/html": "739588",
    "text/plain": "137335",
    "text/troff": "2",
    "text/x-diff": "1",
    "text/x-jsp": "3",
    "text/x-perl": "14",
    "text/x-php": "25",
    "text/x-python": "5",
    "text/x-vcard": "19",
    "video/mp4": "675",
    "video/mpeg": "255",
    "video/quicktime": "954",
    "video/x-flv": "13",
    "video/x-m4v": "203",
    "video/x-ms-asf": "26",
    "video/x-ms-wmv": "139",
    "video/x-msvideo": "96",
    "xscapplication/zip": "85"
}
   </pre>  
</ul>
</div>
</div>

<div class="gdlinebox">
<h4><a name="cca">2. CCA generic schema</a></h4>
<br>
The documents in each dataset are stored in a structured format called the Common Crawl Architecture (CCA), developed as part of the DARPA MEMEX project.  Records in this schema follow this format:
<br>
<pre style="font-family: monospace">
{   'key': 'ebola-03cad6ee34e9dc0aeb77e4c5d31aad2aa41f6ad819f23b8504612d6e6de8a18c',
    'request': {   'body':    None,
                   'client':  { '...': '...' },
                   'headers': [ [ 'Accept-Language': 'en-US,en' ], [ '...', '...' ] ],
                   'method':  'GET'
    },
    'response': {  'body':    '&lt;!DOCTYPE html&gt; &lt;html lang=\'en\' class=\'js-disabled\'&gt; &lt;head&gt; ... &lt;/html&gt;',
                   'status':  '200',
                   'headers': [ ['Content-Type', 'text\/html' ], [ '...', '...'] ],
    },
    'timestamp': 1421064000L,
    'url': 'http://www.nature.com/news/ebola-1.15750',
    'indices': [
        { 'key': 'crawl', 'value': 'ebola' },
        { 'key': '...', 'value': '...'},
    ],
    'features': [
        { '...': '...' }, { '...': '...' }
    ],
}
</pre>
<br>
More or fewer fields are present in the different datasets depending on how they were created.  Every record in every dataset has a 'key' field (the "document number"), a 'response.body' with raw content, the timestamp, and the source URL.  They should also have an 'indices.key=crawl' field indicating which dataset the document goes with.  The 'features' block is meant to contain extracted or derived data.  In the Ebola tweets subset to hold a structured representation of the tweet, automatically extracted from the raw Twitter HTML contained in the 'response.body'.  In the Illicit-goods dataset, the features block contains extracted posts and thread/post metadata from the raw HTML thread.
</div>

<div class="gdlinebox">
<h4><a name="format">3. Data format</a></h4>
<br>
The documents are stored in files that each contain a stream of <a href="http://cbor.io/">CBOR</a> records that follow the CCA format above.  CBOR is a variation of JSON that supports binary data and a has more efficient encoding than text.

Here is an example <a href="cbor-python-example.html">using Python and the cbor library</a>.  More example code in Python and Java can be found at <a href="https://github.com/trec-dd/trec-dd-example-code">the TREC DD Github site</a>.
</div>

<div class="gdlinebox">
<h4><a name="get">4. Obtaining the datasets</a></h4>
<br>
To get access to the collections, you must be a TREC 2016 participant.  Complete the <a href="http://trec.nist.gov/act_part/tracks/dd/TREC DD Organization User Agreement.html">TREC DD Organizational User Agreement</a> (password protected, use the TREC participant password) and email it as a scanned PDF or a high-quality digital photo to Angela (dot) Ellis (at) NIST (dot) gov, including your TREC participant ID.  Within a week you should receive access credentials to download the data, as well as the decryption key to read the local-politics documents.
<br>
Local users of the data must complete the <a href="http://trec.nist.gov/act_part/tracks/dd/TREC DD Individual User Agreement.html">Individual User Agreement</a> and return it to the organizational point of contact who will maintain those records.
</div>
            </section>

            <div class="footerline"></div>

            <footer>
				This page is owned by TREC Dynamic Domain Track Group.
			</footer>
      </div>
	</body>
</html>

